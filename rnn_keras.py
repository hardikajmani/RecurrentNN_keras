# -*- coding: utf-8 -*-
"""RNN_keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e9ebiJqDmx9hbH0fpypux6oJU4vUPxcE

This a rnn with keras used for doing  sentiment analysis on tweets
There is a dataset of 16000 lines and the task is to describe  each tweet as positive or negative.
Although data is here and will also be uploaded with the code.
Here's the link to download the dataset :
https://github.com/crwong/cs224u-project/tree/master/data/sentiment
"""

#importing the required libraries

import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.layers.convolutional import Conv1D
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import pandas as pd
import numpy as np

!pip3 install spacy
!python -m spacy download en

# read after completion
import spacy

nlp=spacy.load("en")



#load the dataset

#uploading the dataset

from google.colab import files
uploaded = files.upload()


#!wget -c https://raw.githubusercontent.com/crwong/cs224u-project/master/data/sentiment/training.1600000.processed.noemoticon.csv -O twitter.csv

#import the dataset

train = pd.read_csv("twitter.csv", encoding = "latin - 1")

y = train[train.columns[0]]
X = train[train.columns[5]]

'''print(X)

print(y)
'''

# split the data into train and test sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size = 0.2, random_state = 42)
y_test = pd.get_dummies(y_test) #one hot encoding ke similar

#function to remove stopwords

def stopwords(sent):
  new = []
  sent = nlp(sent)
  for w in sent:
    if(w.is_stop == False) & (w.pos_ != "PUNCT"):
      new.append(w.string.strip())
    c = " ".join(str(x) for x in new)
  return c

# function to lemmatize the tweets
def lemma(sent):
  sent = nlp(sent)
  str = ""
  for w in sent:
    str += " " + w.lemma_
  return nlp(str)

#loading the glove model (basically a step in vectorizing the corpus)

def loadGloveModel(gloveFile):
  print("Loading Glove Model")
  
  f = open(gloveFile, 'r')
  model = {}
  for line in f:
    splitLine = line.split()
    word = splitLine[0]
    embedding = [float(val) for val in splitLine[1:]]
    model[word] = embedding
  print("Done."),len(model), (" words loaded!")
  return model

"""I have downloaded the glove file on the virtual machine instead of doing it on the local machine. And later extracted that using zipfile"""

!wget "http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip"

from zipfile import ZipFile

file_name = "glove.twitter.27B.zip"

with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print("Done")

# save the glove model

model = loadGloveModel("glove.twitter.27B.200d.txt")

#vectorizing the sentences
def sent_vectorizer(sent, model):
  
  sent_vec = np.zeros(200)
  numw = 0
  for w in sent.split():
    try:
      
      sent_vec = np.add(sent_vec, model[str(w)])
      numw += 1
    except:
      pass
    return sent_vec

#obtain a clean vector

cleanvector = []
for i in range(X_test.shape[0]):
  document = X_test[i]
  document = document.lower()
  document = lemma(document)
  document = str(document)
  cleanvector.append(sent_vectorizer(document, model))

#Getting the input and output in proper shape

cleanvector = np.array(cleanvector)
cleanvector = cleanvector.reshape(len(cleanvector), 200, 1)

#tokenizing the sequences

tokenizer = Tokenizer(num_words=16000)
tokenizer.fit_on_texts(X_test)
sequences = tokenizer.texts_to_sequences(X_test)
word_index = tokenizer.word_index
print('Found %s unique tokens' %len(word_index))
data = pad_sequences(sequences, maxlen = 15, padding = 'post')
print(data.shape)

#reshape the data and prepair to train
data = data.reshape(len(cleanvector), 15, 1)
from sklearn.model_selection import train_test_split
trainx, validx, trainy, validy = train_test_split(data, y_test, test_size = 0.3, random_state = 42)

#calculate the number of words
nb_words = len(tokenizer.word_index) + 1
print(nb_words)

#obtain the embedding matrix

embedding_matrix = np.zeros((nb_words, 200))
for word, i in word_index.items():
  embedding_vector = model.get(word) #word ki vector representation mil rahi h
  if embedding_vector is not None:
    embedding_matrix[i] = embedding_vector
print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis = 1) == 0))

#converting the raininf data to numpy array

trainy = np.array(trainy)
validy = np.array(validy)

#building a simple rnn network
def modelbuild():
  model = Sequential()
  model.add(keras.layers.InputLayer(input_shape=(15,1)))
  keras.layers.embeddings.Embedding(nb_words, 15, weights = [embedding_matrix], input_length= 15, trainable = False)
  
  model.add(keras.layers.recurrent.SimpleRNN(units = 500, activation = 'relu', use_bias = True))
  model.add(keras.layers.Dense(units = 1000, input_dim = 2000, activation = 'sigmoid'))
  model.add(keras.layers.Dense(units = 500, input_dim = 1000, activation = 'relu'))
  model.add(keras.layers.Dense(units = 2, input_dim = 500, activation = 'softmax'))
  model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
  
  return model

#compiling the model
final_model = modelbuild()
final_model.fit(trainx, trainy, epochs = 200, batch_size = 120, validation_data = (validx, validy))

#getting the accuracy on validation set on last epoch

loss, acc = final_model.evaluate(validx, validy, verbose = 0)
print("Accuracy: " + str(acc * 100))